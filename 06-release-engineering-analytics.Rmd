# Release Engineering Analytics

## Motivation

Release engineering is a discipline involved with making software available for
end users.
Efforts spent within the development environment of a software system
should eventually be integrated and deployed such that end users may benefit
from them.
In recent years, release engineers have developed and
adopted techniques to build infrastructures and pipelines which automate the
process of releasing software to an increasingly large degree.
These modern approaches have resulted
in various practices such as releasing new versions of a software system in
significantly shorter cycles.

Due to these developments being industry-driven, release engineering forms a
largely uncharted territory for software engineering research.
It requires the attention from researchers both because these new practices
have an often unanticipated impact on software studies and because they require
empirical validation [@adams2016a].

Therefore, this systematic literature review aims to provide an overview of
the software analytics research that has been conducted so far on release
engineering. Its main purpose is to identify the apparent gap
between research and practice, in order to guide further research efforts.

### Research Questions

Contrary to what is regularly the case, with release engineering, practice
seems to be ahead of research.
Building on this idea, our questions are constructed to identify in which ways
existing modern release engineering practices should still be studied in
software analytics research.
Our review thus aims to answer the following questions.

<!-- TODO -->
- **RQ 1:** _How is modern release engineering done in practice?_<br>
  This question aims to identify the so-called "state of the practice" in
  release engineering.
  We will summarize practices that have been adopted to drive release
  engineering forward. In addition we will identify the tools utilized to bring
  this about. Case studies will also be analyzed to this end.

- **RQ 2:** _What aspects of modern release engineering have been studied
  in software analytics research so far?_<br>
  In order to answer this question we investigate the practices that previous
  empirical studies have focused on. In doing so, we identify the associated
  costs and benefits that have been found, and the analysis methods used.

- **RQ 3:** _What aspects of modern release engineering make for relevant
  study objects in future software analytics research?_<br>
  In answering this question we aim to identify the gap between practice and
  research in release engineering. This way, our intent is not only to guide
  but also to motivate future research.

## Research Protocol

<!-- TODO -->
In this section, we will describe...

### Search Strategy

Since release engineering is a relatively new research topic,
we took an exploratory approach in collecting any literature revolving around
the topic of release engineering from the perspective of software analytics.
This aided us in determining a more narrow scope for our survey,
subsequently allowing us to find additional literature fitting this scope.

At the start of this project, we were provided with an initial seed of five
papers as a starting point for our literature survey.
These initial papers were @adams2016a, @da2016a, @d2014a, @khomh2012a, and
@khomh2015a.

We collected publications using two search engines: Scopus and Google Scholar.
These each encompass various databases such as ACM Digital Library, Springer,
IEEE Xplore and ScienceDirect.
The main query that we constructed is displayed in Figure 1.
The publications found using this query were:

- @kaur2019a
- @kerzazi2013a
- @castelluccio2017a
- @karvonen2017a
- @claes2017a
- @fujibayashi2017a
- @souza2015a
- @laukkanen2018a

```
TITLE-ABS-KEY(
  (
    "continuous release" OR "rapid release" OR "frequent release"
    OR "quick release" OR "speedy release" OR "accelerated release"
    OR "agile release" OR "short release" OR "shorter release"
    OR "lightning release" OR "brisk release" OR "hasty release"
    OR "compressed release" OR "release length" OR "release size"
    OR "release cadence" OR "release frequency"
    OR "continuous delivery" OR "rapid delivery" OR "frequent delivery"
    OR "fast delivery" OR "quick delivery" OR "speedy delivery"
    OR "accelerated delivery" OR "agile delivery" OR "short delivery"
    OR "lightning delivery" OR "brisk delivery" OR "hasty delivery"
    OR "compressed delivery" OR "delivery length" OR "delivery size"
    OR "delivery cadence" OR "continuous deployment" OR "rapid deployment"
    OR "frequent deployment" OR "fast deployment" OR "quick deployment"
    OR "speedy deployment" OR "accelerated deployment" OR "agile deployment"
    OR "short deployment" OR "lightning deployment" OR "brisk deployment"
    OR "hasty deployment" OR "compressed deployment" OR "deployment length"
    OR "deployment size" OR "deployment cadence"
  ) AND (
    "release schedule" OR "release management" OR "release engineering"
    OR "release cycle" OR "release pipeline" OR "release process"
    OR "release model" OR "release strategy" OR "release strategies"
    OR "release infrastructure"
  )
  AND software
) AND (
	LIMIT-TO(SUBJAREA, "COMP") OR LIMIT-TO(SUBJAREA, "ENGI")
)
AND PUBYEAR AFT 2014
```

_Figure 1. Query used for retrieving release engineering publications via Scopus._

In addition to querying search engines as described above,
references related to retrieved papers were analyzed.
For each paper, the review concerned the publications cited by the paper,
as well as those citing the paper.
These reference lists were obtained from Google Scholar and from the reference
lists in the papers themselves.
The results of the reference analysis are listed in Table 1.

_Table 1. Papers found indirectly by investigating citations of/by other papers._

| Starting point | Type        | Result                              |
|----------------|-------------|-------------------------------------|
| @souza2015a    | has cited   | @plewnia2014a   <br> @mantyla2015a  |
| @khomh2015a    | is cited by | @poo-caama2016a <br> @teixeira2017a |
| @mantyla2015a  | is cited by | @rodriguez2017a <br> @cesar2017a    |

All the papers that were found, were stored in a custom built web-based tool for
conducting literature reviews.
The source code of this tool is published in a
[GitHub repository](https://github.com/jessetilro/research).
The tool was hosted on a virtual private server, such that all retrieved
publications were stored centrally, accessible to all reviewers.

### Study Selection

In the utilized tool for conducting the survey, it is possible to label papers
with tags and leave comments and ratings.
Every paper is reviewed based on the selection criteria.
Based on this, the tool allowed to filter out all papers
that appeared not to be relevant for this literature survey.

The selection criteria are as follows:

1. The study must show (at least) one release engineering technique.
2. The study must not just show a release engineering technique,
   but analyze its performance compared to other techniques.

Based on these selection criteria,
the following papers appeared to be irrelevant for the scope of this survey:

<!-- Example. TODO: actually exclude papers -->
- [link to paper] - Excluded based on rule 2.

### Study Quality Assessment

Based on @kitchenham2004procedures, the quality of a paper will be assessed
by the evidence it provides, based on the following scale.
All levels of quality will be accepted, except for level 5 (expert opinion).

1. Evidence obtained from at least one properly-designed
   randomised controlled trial.
2. Evidence obtained from well-designed pseudo-randomised controlled trials
   (i.e. non-random allocation to treatment).
3. Comparative studies in a real-world setting:
    1. Evidence obtained from comparative studies with concurrent controls and
       allocation not randomised, cohort studies, case-control studies or
       interrupted time series with a control group.
    2. Evidence obtained from comparative studies with historical control,
       two or more single arm studies,
       or interrupted time series without a parallel control group.
4. Experiments in artificial settings:
    1. Evidence obtained from a randomised experiment performed in an
       artificial setting.
    2. Evidence obtained from case series,
       either post-test or pre-test/post-test.
    3. Evidence obtained from a quasi-random experiment performed in an
       artificial setting.
5. Evidence obtained from expert opinion based on theory or consensus.

Also, the studies will be examined to see if they contain any type of bias.
For this, the same types of biases will be used as described by @kitchenham2004procedures:

- Selection/Allocation bias: Systematic difference between comparison groups
  with respect to treatment.
- Performance bias: Systematic difference is the conduct of comparison groups
  apart from the treatment being evaluated.
- Measurement/Detection bias: Systematic difference between the groups in how
  outcomes are ascertained.
- Attrition/Exclusion bias: Systematic differences between comparison groups in
  terms of withdrawals or exclusions of participants from the study sample.

The studies will be labeled by their quality level and possible biases.
This information can be used during the Data Synthesis phase
to weigh the importance of individual studies [@kitchenham2004procedures].

### Data Extraction
To accurately capture the information contributed by each publication in our
survey, we will use a systematic approach to extracting data.
To guide this process, we will be using a data extraction form which describes
what aspects of a publication are crucial to record.
Besides general publication information (title, author etc.), the form contains
questions that are based on our defined research questions.
Furthermore, the form contains a section for quantitative research, where
aspects such as population and evaluation will be documented.
The form that is used for this is shown below:

```
General information:
	Name of person extracting data:
	Date form completed (dd/mm/yyyy):
	Publication title:
	Author information:
	Journal:
	Publication type:
	Type of study:

What practices in release engineering does this publication mention?

Are these practices to be classified under dated, state of the art or state of
the practice? Why?

What open challenges in release engineering does this publication mention?

What research gaps does this publication contain?

Are these research gaps filled by any other publications in this survey?

Quantitative research publications:
	Study start date:
	Study end date or duration:
	Population description:
	Method(s) of recruitment of participants:
	Sample size:
	Evaluation/measurement description:
	Outcomes:
	Limitations:
	Future research:

Notes:

```

### Data Synthesis
To summarize the contributions and limitations of each of the included
publications, we will apply a descriptive synthesis approach.
In this part of our survey, we will compare the data that was extracted of the
included publications.
Publications with similar findings will be grouped and evaluated, and
differences between groups of publications will be structured and elaborated on.
In this we will compare them using specifics such as their study types, time of
publication and study quality.

If the extracted data allows for a structured tabular visualization of
similarities and differences between publications this we serve as an additional
form of synthesis. However, this depends on the final included publications of
this survey.

### Included and Excluded Studies

### Project timetable

The literature review was conducted over the course of four weeks. We worked
iteratively and planned for four weekly milestones.

| Milestone | Deadline        | Goals                              |
|----------------|-------------|-------------------------------------|
| Milestone 1    | 16/9/18  | - Develop the search strategy<br> - Collect initial publications  |
| Milestone 2    | 23/9/18 | Write full research protocol |
| Milestone 3  | 30/9/18 | - Collect additional literature according to the protocol<br> - Perform data extraction    |
| Milestone 4  | 7/10/18 | - Perform data synthesis<br> - Write final version of the chapter |

<!-- TODO -->

## Answers

<!-- TODO -->

### RQ1: ...

<!-- TODO -->

### RQ2: ...

<!-- TODO -->

### RQ3: ...

<!-- TODO -->

## Discussion

<!-- TODO -->

## Conclusion

<!-- TODO -->

## Appendix

<!-- TODO Maarten -->

<!-- TODO Jesse -->

## Modern Release Engineering in a Nutshell
General information: 
	Name of person extracting data: Nels Numan
	Date form completed (dd/mm/yyyy): 28/09/2018
	Publication title: Modern Release Engineering in a Nutshell
	Author information: Bram Adams and Shane McIntosh
	Journal: 23rd International Conference on Software Analysis, Evolution, and Reengineering (2016)
	Publication type: Conference paper
	Type of study: Survey

What practices in release engineering does this publication mention?
- Branching and merging
	- Software teams rely on Version Control Systems
	- Quality assurance activities like code reviews are used before doing a merge or even allowing a code change to be committed into a branch
	- Keep branches short-lived and merge often. If this is impossible, a rebase can be done.
	- "trunk-based development" can be applied to eliminate most branches below the master branch.
	- Feature toggles are used to provide isolation for new features in case of the absence of branches.
- Building and testing
	- To help assess build and test conflicts, many projects also provide "try" servers to development teams, which automatically runs a build and test process referred to as CI.
	- The CI process often does not run full test, but a representative subset.
	- The more intensive tests, such as integration, system or performance typically get run nightly or in weekends.
- Build system:
	- GNU Make is the most popular file-based build system technology. Ant is the prototypical task-based build system technology. Lifecycle-based build technologies like Maven consider the build system of a project to have a sequence of standard build activities that together form a "build lifecycle."
	- "Reproducible builds" involve for a given feature and hardware configuration of the code base, every build invoca- tion should yield bit-to-bit identical build results.
- Infrastructure-as-code
	- Containers or virtual machines are used to deploy new versions of the system for testing or even production.
	- It has been recommended that infrastructure code is to be stored in a separate VCS repository than source code, in order to restrict access to infrastructure code.
- Deployment
	- The term "dark launching" corresponds to deploying new features without releasing them to the public, in which parts of the system automatically make calls to the hidden features in a way invisible to end users.
	- "Blue green deployment" deploys the next software version on a copy of the production environment, and changes this to be the main enviroment on release.
	- In "canary deployment" a prospective release of the software system is loaded onto a subset of the production environments for only a subset of users.
	- "A/B testing" deploys alternative A of a feature to the environment of a subset of the user base, while alternative B is deployed to the environment of another subset.
- Release
	- Once a deployed version of a system is released, the release engineers monitor telemetry data and crash logs to track the performance and quality of releases. Several frameworks and applications have been introduced for this.

Are these practices to be classified under dated, state of the art or state of
the practice? Why?
The majority of these practices are classified by the paper as state of the practice, but state of the art practices are also mentioned.

What open challenges in release engineering does this publication mention?
- Branching and merging
	- No methodology or insight exists on how to empirically validate the best branching structure for a given organization or project, and what results in the smallest amount of merge conflicts.
	- Release engineers need to pay particular attention to conflicts and incompatibilities caused by evolving library and API dependencies.
- Building and testing
	- Speeding up CI might be the major concern of practitioners. This speed up can be achieved through predicting whether a code change will break the build, or by "chunking" code changes into a group and only compile and test each group once.
	- The concept of "green builds" slowly is becoming an issue, in the sense that frequent triggering of the CI server consumes energy.
	- Security of the release engineering pipeline in general, and the CI server in particular, also has become a major concern.	
- Release
	- Qualitative studies are not only essential to understand the rationale behind quantitative findings, but also to identify design patterns and best practices for build systems.
		- How can developers make their builds more maintainable and of higher quality?
		- What refactorings should be performed for which build system anti-patterns?
	- Identification and resolution of build bugs, i.e., source code or build specification changes that cause build breakage, possibly on a subset of the supported platforms.
	- Basic tools have a hard time determining what part of the system is necessary to build.
	- Studies on non-GNU Make build systems are missing.
	- Apart from identifying bottlenecks, such approaches should also suggest concrete refactorings of the build system specifications or source code.
- Infrastructure-as-code
	- Research on differences between infrastructure languages is lacking.
	- Best practices and design patterns for infrastructure-as-code need to be documented.
	- Qualitative analysis of infrastructure code will be necessary to understand how developers address different infrastructure needs.
	- Quantitative analysis of the version control and bug report systems can then help to determine which patterns were beneficial in terms of maintenance effort and/or quality.
- Deployment
	- More emperical studies can be done to answer question like this:
		- Is blue-green deployment the fastest means to deploy a new version of a web app?
		- Are A/B testing and dark launching worth the investment and risk?
		- Should one use containers or virtual machines for a medium-sized web app in order to meet application performance and robustness criteria?
		- If an app is part of a suite of apps built around a common database, should each app be deployed in a different container?
	- Better tools for quality assurance are required, to prevent showstopper bugs from slipping through and requiring re-deployment of a mobile app version (with corresponding vetting), these include:
		- Defect prediction (either file- or commit-based)
		- Smarter/safer update mechanisms
		- Tools for improving code review
		- Generating tests
		- Filtering and interpreting crash reports
		- Prioritization and triaging of defect reports
- Release
	- More research is needed on determining which code change is the perfect one for triggering the release of one of these releases, or whether a canary is good enough to be released to another data centre.
	- Question such as the following should be investigated:
		- Should one release on all platforms at the same time? 
		- In the case of defects, which platform should receive priority? 
		- Should all platforms use the same version numbering, or should that be feature-dependent?
		- Research on the continuous delivery and rapid releases from other systems should be explored.

What research gaps does this publication contain?
As is common with surveys, it does not contain the state of the field today. More quantitive and qualitive research has been done, which can not possibly be included.

Are these research gaps filled by any other publications in this survey?
An example of further research that expand on this study is @da2016a

## 
General information: 
	Name of person extracting data: Nels Numan
	Date form completed (dd/mm/yyyy): 28/09/2018
	Publication title: The Impact of Switching to a Rapid Release Cycle on the Integration Delay of Addressed Issues
	Author information: Daniel Alencar da Costa, Shane McIntosh, Uira Kulesza, Ahmed E. Hassan
	Journal: 13th Working Conference on Mining Software Repositories (2016)
	Publication type: Conference paper
	Type of study: Emperical study

What practices in release engineering does this publication mention?
To give a context to the study, the paper describes the concept of traditional releases, rapid releases, their differences, and how issue reports are structured.

Are these practices to be classified under dated, state of the art or state of
the practice? Why?
State of the practice. The paper describes common practices that were in use at the time of the publication.

What open challenges in release engineering does this publication mention?
The study mentions that comparing systems with different release structures is difficult since one has to distinguish to what extent the results are due to the release strategy and which are due to intricacies of the systems or organization itself.

What research gaps does this publication contain?
The main gap in this study is the specificity of the data. Only Mozilla has been considered, and external factors such as other organizational challenges which could have an effect on release time could not be included. More research that looks further into comparing this case to that of other organizations is needed.

Are these research gaps filled by any other publications in this survey?


Quantitative research publications:
	Study start date: Used data starts from 1999
	Study end date or duration: Used data ends in 2010
	Population description: The paper describes multiple steps to describe their data collection approach. The paper collected the date and version number of each Firefox release. Tags within the VCS were used to link issue IDs to releases. The paper discards issues that are potential false positives: IDs that have less five digits, issues that refer to tests instead of bugfixes, any potential ID that is the name of a file. Since the commit logs are linked to the VCS tags, the paper is able to link the issue IDs found within these commit logs to the releases that correspond to those tags.
	Method(s) of recruitment of participants: Firefox release history wiki and VCS logs
	Sample size: 72114 issue reports from the Firefox system (34673 for traditional releases and 37441 for rapid releases)
	Evaluation/measurement description: The paper aims to answer three research questions:
      - Are addressed issues integrated more quickly in rapid releases?
      Approach: Through beanplots to compare the distributions, the paper first observes the lifetime of the issues of traditional and rapid releases. Next, it looks at the time span of the triaging, fixing, and integration phases within the lifetime of an issue.
      - Why can traditional releases integrate addressed issues more quickly?
      Approach: the paper groups traditional and rapid releases into major and minor releases and study their integration delay through beanplots, Mann-Whiteney-Wilcoxon tests, Cliff's delta, and MAD.
      - Did the change in the release strategy have an impact on the characteristics of delayed issues?
      Approach: the paper builds linear regression models for both release approaches. The paper firstly estimates the degrees of freedom that can be spent on the models. Secondly, they check for metrics that are highly correlated using Spearman rank correlation tests and perform a redundancy check to remove redundant metrics. The paper then assesses the fit of our models using the ROC area and the Brier score. The ROC area is used to evaluate the degree of discrimination achieved by the model. The Brier score is used to evaluate the accuracy of probabilistic predictions. The used metrics include reporter experience, resolver experience, issue severity, issue priority, project queue rank, number of impacted files and fix time. A full list of metrics can be found in Table 2 of the paper.
	Outcomes:
    - Are addressed issues integrated more quickly in rapid releases?
    Results: There is no significant difference between traditional and rapid releases regarding issue lifetime.
    Results: 
    - Why can traditional releases integrate addressed issues more quickly?
    Results: Minor-traditional releases tend to have less integration delay than major/minor-rapid releases.
    - Did the change in the release strategy have an impact on the characteristics of delayed issues?
    Results: The models achieve a Brier score of 0.05- 0.16 and ROC areas of 0.81-0.83. Traditional releases prioritize the integration of backlog issues, while rapid releases prioritize the inte- gration of issues of the current release cycle. 
	Limitations: Defects in the tools that were developed to perform the data collection and evaluation could have an effect on the outcomes. Furthermore, the way that issue IDs are linked to releases may not represent the total addressed issues per release. The results cannot be generalized as the evaluation was solely done on the Firefox system.
	Future research: Further research can look into applying the same evaluation strategy to other organizations that switched from traditional to rapid release.

Notes:

## 
General information:
  Name of person extracting data: Nels Numan
  Date form completed (dd/mm/yyyy): 29/09/18
  Publication title: An Empirical Study of Delays in the Integration of Addressed Issues
  Author information: Daniel Alencar da Costa, Surafel Lemma Abebe, Shane McIntosh, Uira Kulesza, Ahmed E. Hassan
  Journal: 2014 IEEE International Conference on Software Maintenance and Evolution
  Publication type: Conference paper
  Type of study: Emperical study

What practices in release engineering does this publication mention?
This publication discusses the usage of issue tracking systems, and what the term issue means to form a context around the study.

Are these practices to be classified under dated, state of the art or state of
the practice? Why?
State of the practice.

What open challenges in release engineering does this publication mention?
The results based on the investigated open source projects may not be generalizable and replication of the study is required on a larger set of projects to form a more general conclusion. Another challenge is finding metrics that are truly correlated with the integration delay of issues.
What research gaps does this publication contain?
Please see last question.
Are these research gaps filled by any other publications in this survey?
@da2016a

Quantitative research publications:
  Study start date: 
  Used data start dates:
      - ArgoUML: 18/08/2003
      - Eclipse: 03/11/2003
      - Firefox: 05/06/2012
  Used data end dates:
      - ArgoUML: 15/12/2011
      - Eclipse: 12/02/2007
      - Firefox: 04/02/2014  
  Population description:
  Method(s) of recruitment of participants: The data was collected from both ITSs and VCSs of the studied systems.
  Sample size: 20,995 issues from ArgoUML, Eclipse and Firefox projects
  Evaluation/measurement description:
      - How long are addressed issues typically delayed by the integration process?
      Approach: models are created using metrics from four dimensions: reporter, issue, project, and history. Please refer to Table 2 in the paper for all of the metrics considered. The models are trained using the random forest technique. Precision, recall, F-measure, and ROC area are used to evaluate the models.
  Outcomes:
      - How long are addressed issues typically delayed by the integration process?
      Addressed issues are usually delayed in a rapid release cycle. Many delayed issues were addressed well before releases from which they were omitted. Many delayed issues were addressed well before releases from which they were omitted.
      - Can we accurately predict when an addressed issue will be integrated? 
      The prediction models achieve a weighted average precision between 0.59 to 0.88 and a recall between 0.62 to 0.88, with ROC areas of above 0.74. The models achieve better F-measure values than Zero-R.
      - What are the most influential attributes for estimating integration delay? The integrator workload has a bigger influence on integrator delay than the other attributes. Severity and priority have little influence on issue in- tegration delay.
  Limitations: See open challenges.
  Future research: See open challenges.

Notes:

## Towards Definitions for Release Engineering and DevOps
General information:
  Name of person extracting data: Nels Numan
  Date form completed (dd/mm/yyyy): 30/09/2018
  Publication title: Towards Definitions for Release Engineering and DevOps
  Author information: Andrej Dyck, Ralf Penners, Horst Lichter
  Journal:
  Publication type:
  Type of study: Survey

What practices in release engineering does this publication mention?
This paper talks about approaches to improve the collaboration between development and IT operations teams, in order to streamline software engineering processes. The paper defines for release engineering and devops.

Are these practices to be classified under dated, state of the art or state of
the practice? Why?
Not applicable.

What open challenges in release engineering does this publication mention?
The paper mentions that creating a definition which is uniform and valid for many situations is difficult to find and that further research is needed.

What research gaps does this publication contain?
This paper aims to form a uniform definition for release engeneering and devops, in collaboration with experts. It is unclear how many experts were consulted for this definition, and more consultations and research could be done to further improve the definition.

Are these research gaps filled by any other publications in this survey?

Quantitative research publications:
  Study start date:
  Study end date or duration:
  Population description:
  Method(s) of recruitment of participants:
  Sample size:
  Evaluation/measurement description:
  Outcomes:
  Limitations:
  Future research:

Notes: