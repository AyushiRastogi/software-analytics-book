--- 
title: "A Literature Survey of Software Analytics"
author: "Moritz Beller, IN4334 2018 TU Delft"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
github-repo: saltudelft/software-analytics-book
description: "This is a contemporary, systematic survey of literature on Software Analytics."
---

# Preamble {#intro}

The book you see in front of you is the outcome of an eight week seminar run by the Software Engineering Research Group (SERG) at TU Delft. We have split up the novel area of Software Analytics into several sub topics. Every chapter addresses one such sub-topic of Software Analytics and is the outcome of a systematic literature review a laborious team of 3-4 students performed.

With this book, we hope to structure the new field of Software Analytics and show how it is related to many long existing research fields.

*Moritz Beller*



## License

![Creative Commons](figures/cc-nc-sa.png)
This book is copyrighted 2018 by TU Delft and its
respective authors and distributed under a [CC BY-NC-SA 4.0
license](https://creativecommons.org/licenses/by-nc-sa/4.0/)

<!--chapter:end:index.Rmd-->

# A contemporary view on Software Analytics

## What is Software Analytics?

## A list of Software Analytics Sub-Topics

<!--chapter:end:01-definition.Rmd-->

# Sample Sub-Topic

This is an example for the deliverable every group works on. Every group works on one independent chapter (starting as one Rmd file).

<!--chapter:end:02-sample.Rmd-->

# Final Words

We have finished a nice book on Software Analytics.

<!--chapter:end:03-summary.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:04-references.Rmd-->

# Runtime and Performance Analytics

In this chapter, we discuss the field of performance and runtime analytics. This chapter does not 
cover the entire field because it is too broad. Using Kitchenham’s method 
[@kkitchenham2004procedures], we have narrowed down the scope of this survey. 

For inspiration, we started reading five recent papers on runtime and performance analytics 
published at top conferences. These five were selected because the papers handle the software side 
of performance and runtime analytics which is more in line with the other chapters of this book. 
However, focussing on only software, the field is still very broad. Currently, we are leaning 
towards a focus on performance and runtime analytics literature regarding the Android platform. 
As we still are at the start of this research, we might deviate from this initial focus.

We have gathered a few other papers (excluding the five initial papers) to find out if this field 
is suited for this survey. These papers can be found in Figure INSERT FIGURE NUMBER HERE OR SOMETHING.
To get relevant papers, we used the following keywords: Android, performance, runtime, reliability, 
synchronization, security, monitoring. Furthermore, we only retrieved papers published at top 
venues, which we list here:

* ACM Transactions on Software Engineering Methodology (TOSEM), 
* Empirical Software Engineering (EMSE), 
* IEEE Transactions on Software Engineering (TSE), 
* Information and Software Technology (IST), 
* Journal of Systems and Software (JSS), 
* ACM Computing Surveys (CSUR),
* Foundations of Software Engineering (SIGSOFT FSE),
* International Conference on Automated Software Engineering (ASE),
* Working Conference on Mining Software Repositories (MSR) 
* Symposium on Operating Systems Design and Implementation (OSDI)

Because we consider the five starting papers to be our inspiration, we have chosen to briefly 
describe these papers by giving some basic metrics about them (citations), summarizing them and by 
adding a few notes about them. This is our initial work that we would like to expand on in the 
coming weeks.


## Charting the API minefield using software telemetry data

In this paper, researchers used software telemetry data from mobile application crashes. With 
heuristics, they separated the API calls from application calls so they can analyze what the most 
common causes for crashes are. Top crash causes are: memory exhaustion, race conditions or 
deadlocks, and missing resources. A significant percentage was not suitable for analysis as these 
crashes were associated with generic exceptions (10%). They performed a literature search to find 
solutions to the problems that cause the crashes. For each crash cause category, an implementation 
recommendation is made. More specific exceptions, non-blocking algorithms, and default resources 
can eliminate the most frequent crashes. They also suggest that development tools like memory 
analyzers, thread debuggers, and static analyzers can prevent many application failures. They also 
propose features of execution platforms and frameworks related to process and memory management 
that could reduce application crashes.

### Remarks

* Among the papers that refer to this paper or are referenced by this paper there are four papers 
that share the topic of crash data on mobile platforms that have been publiced to top software 
engineering venues []. 
* The paper seems to be quite discerning as they evaluate their methods and reason about the 
threats to validity. 
  

## Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring

The mobile applications market continues to grow and many applications are available. It is important for developers that their application keeps working and that crashes are fixed as fast as possible to keep up with competitors. However, the mobile market is complex as for end users there are endless configurations of application versions, mobile hardware and  user input sequences. Therefore, it is difficult to reproduce software crashes under the same context and conditions that triggered the observed crash. This is why the researchers developed MoTiF which uses machine learning to reproduce the steps the end users take before the app crashes on the end user’s phone and generates a test suite. MoTiF also uses the crowd to validate whether the generated test suite truly reproduces the observed crash.

### Remarks

* The datasets used for the research are a bit questionable. One is based on simply performing a large amount of random event on the app, the other dataset is created by letting a group of 10 student try to crash the app in one hour. 
* Only 5 different apps have been tested. 
* Contains reference to "Charting the API minefield using software telemetry data". 

## An exploratory study on faults in web api integration in a large-scale payment company

This research explores what the implications of web API faults are, what the most common web API faults are and best practices for API design. The faults in API integration can be grouped in 11 causes: invalid user input, missing user input, expired request data, invalid request data, missing request data, insufficient permissions, double processing, configuration, missing server data, internal and third party. Most faults can be attributed to the invalid or missing request data, and most API consumers seem to be impacted by faults caused by invalid request data and third party integration. Furthermore, API consumers most often use official API documentation to implement an API correctly, followed by code examples. The challenges of preventing runtime problems are the lack of implementation details, insufficient guidance on certain aspects of the integration, insufficient understanding of the impact of problems, and missing guidance on how to recover from errors.

### Remarks

* Easy to read
* Paper only considers a single API
* Survey only has 40 responses

## Search-based test data generation for SQL queries


SQL queries should be tested as thoroughly as program code. However, it is hard to generate test data for testing. Other researchers proposed viewing this problem as a constraint solving problem, so test data could be generated with a SAT-solver. However, strings are not supported by current SAT-solver tools and it is a complex task to translate a query to a satisfiability problem. In this research, the test generation problem is treated as a search-based problem. They use random search, biased random search and genetic algorithms (GA) to generate the data. The methods are combined in a tool called EvSQL and the tool is tested on more than 2000 queries. The GA method is the best and is able to cover a little over 98% of the queries. 

### Remarks

* Easy to read
* Utilizes queries of 4 different systems
* Generation of test data for SQL questies implies easier generation of unit- regression- and integration tests for SQL queries. 

## Anomaly detection using program control flow graph mining from execution logs

The paper attempts to diagnose distributed applications. For this purpose they mine templates and their sequences from exedcution logs, from this information they create a control flow graph. The main cause of failures identified: making an API request to another application. This results in many new calls to other services or even other applications. This flow gets interrupted at some point. So when the top level API is not working, they want to show where it goes wrong. In earlier work, primarily metrics and logs were used to find the cause. However these approaches struggled with many benign warnings or errors in healthy state or faults do not manifest as errors. Manually checking a transaction flow is also very hard. Instead, templates are used as print statements from the source code. These represent the nodes, the edges are the flows. This approach imposes two major challenges. One, mining print statements is hard because parameters are different in every log. Two, flows can happen at the simultaneously. The paper tries to solve these challenges by applying a join on two print statements if the statements are preceded and followed by approximately the same steps. 

### Remarks 

* Has a presentation on YouTube
* Difficult to read

<!--chapter:end:05-runtime-and-performance-analytics.Rmd-->

