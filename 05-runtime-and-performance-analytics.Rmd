# Runtime and Performance Analytics

In this chapter, we discuss the field of performance and runtime analytics. This chapter does not 
cover the entire field because it is too broad. Using Kitchenham’s method 
[@kkitchenham2004procedures], we have narrowed down the scope of this survey. 

For inspiration, we started reading five recent papers on runtime and performance analytics 
published at top conferences. These five were selected because the papers handle the software side 
of performance and runtime analytics which is more in line with the other chapters of this book. 
However, focussing on only software, the field is still very broad. Currently, we are leaning 
towards a focus on performance and runtime analytics literature regarding the Android platform. 
As we still are at the start of this research, we might deviate from this initial focus.

We have gathered a few other papers (excluding the five initial papers) to find out if this field 
is suited for this survey. These papers can be found in Figure INSERT FIGURE NUMBER HERE.
To get relevant papers, we used the following keywords: Android, performance, runtime, reliability, 
synchronization, security, monitoring. Furthermore, we only retrieved papers published at top 
venues, which we list here:

* ACM Transactions on Software Engineering Methodology (TOSEM), 
* Empirical Software Engineering (EMSE), 
* IEEE Transactions on Software Engineering (TSE), 
* Information and Software Technology (IST), 
* Journal of Systems and Software (JSS), 
* ACM Computing Surveys (CSUR),
* Foundations of Software Engineering (SIGSOFT FSE),
* International Conference on Automated Software Engineering (ASE),
* Working Conference on Mining Software Repositories (MSR) 
* Symposium on Operating Systems Design and Implementation (OSDI)

##Week 1

Because we consider the five starting papers to be our inspiration, we have chosen to briefly 
describe these papers by giving some basic metrics about them (citations), summarizing them and by 
adding a few notes about them. This is our initial work that we would like to expand on in the 
coming weeks.


### Charting the API minefield using software telemetry data

In this paper, researchers used software telemetry data from mobile application crashes. With 
heuristics, they separated the API calls from application calls so they can analyze what the most 
common causes for crashes are. Top crash causes are: memory exhaustion, race conditions or 
deadlocks, and missing resources. A significant percentage was not suitable for analysis as these 
crashes were associated with generic exceptions (10%). They performed a literature search to find 
solutions to the problems that cause the crashes. For each crash cause category, an implementation 
recommendation is made. More specific exceptions, non-blocking algorithms, and default resources 
can eliminate the most frequent crashes. They also suggest that development tools like memory 
analyzers, thread debuggers, and static analyzers can prevent many application failures. They also 
propose features of execution platforms and frameworks related to process and memory management 
that could reduce application crashes.

**Remarks**

* Among the papers that refer to this paper or are referenced by this paper there are four papers 
that share the topic of crash data on mobile platforms that have been publiced to top software 
engineering venues []. 
* The paper seems to be quite discerning as they evaluate their methods and reason about the 
threats to validity. 
  

### Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring

The mobile applications market continues to grow and many applications are available. It is 
important for developers that their application keeps working and that crashes are fixed as fast as
possible to keep up with competitors. However, the mobile market is complex as for end users there 
are endless configurations of application versions, mobile hardware and  user input sequences. 
Therefore, it is difficult to reproduce software crashes under the same context and conditions that
triggered the observed crash. This is why the researchers developed MoTiF which uses machine 
learning to reproduce the steps the end users take before the app crashes on the end user’s phone 
and generates a test suite. MoTiF also uses the crowd to validate whether the generated test suite 
truly reproduces the observed crash.

**Remarks**

* The datasets used for the research are a bit questionable. One is based on simply performing a 
large amount of random event on the app, the other dataset is created by letting a group of 10 
student try to crash the app in one hour. 
* Only 5 different apps have been tested. 
* Contains reference to "Charting the API minefield using software telemetry data". 

### An exploratory study on faults in web api integration in a large-scale payment company

This research explores what the implications of web API faults are, what the most common web API 
faults are and best practices for API design. The faults in API integration can be grouped in 11 
causes: invalid user input, missing user input, expired request data, invalid request data, missing
request data, insufficient permissions, double processing, configuration, missing server data, 
internal and third party. Most faults can be attributed to the invalid or missing request data, and
most API consumers seem to be impacted by faults caused by invalid request data and third party 
integration. Furthermore, API consumers most often use official API documentation to implement an 
API correctly, followed by code examples. The challenges of preventing runtime problems are the 
lack of implementation details, insufficient guidance on certain aspects of the integration, 
insufficient understanding of the impact of problems, and missing guidance on how to recover from 
errors.

**Remarks**

* Easy to read
* Paper only considers a single API
* Survey only has 40 responses

### Search-based test data generation for SQL queries


SQL queries should be tested as thoroughly as program code. However, it is hard to generate test 
data for testing. Other researchers proposed viewing this problem as a constraint solving problem, 
so test data could be generated with a SAT-solver. However, strings are not supported by current 
SAT-solver tools and it is a complex task to translate a query to a satisfiability problem. In this
research, the test generation problem is treated as a search-based problem. They use random search,
biased random search and genetic algorithms (GA) to generate the data. The methods are combined in
a tool called EvSQL and the tool is tested on more than 2000 queries. The GA method is the best and
is able to cover a little over 98% of the queries. 

**Remarks**

* Easy to read
* Utilizes queries of 4 different systems
* Generation of test data for SQL questies implies easier generation of unit- regression- and 
integration tests for SQL queries. 

### Anomaly detection using program control flow graph mining from execution logs

The paper attempts to diagnose distributed applications. For this purpose they mine templates and 
their sequences from exedcution logs, from this information they create a control flow graph. The 
main cause of failures identified: making an API request to another application. This results in 
many new calls to other services or even other applications. This flow gets interrupted at some 
point. So when the top level API is not working, they want to show where it goes wrong. In earlier 
work, primarily metrics and logs were used to find the cause. However these approaches struggled 
with many benign warnings or errors in healthy state or faults do not manifest as errors. Manually 
checking a transaction flow is also very hard. Instead, templates are used as print statements from
the source code. These represent the nodes, the edges are the flows. This approach imposes two 
major challenges. One, mining print statements is hard because parameters are different in every 
log. Two, flows can happen at the simultaneously. The paper tries to solve these challenges by 
applying a join on two print statements if the statements are preceded and followed by 
approximately the same steps. 

**Remarks** 

* Has a presentation on YouTube
* Difficult to read

##Week 2

Because we are still working on the exact scope of the survey as well as the lay-out of the chapter, we have chosen to temporarily divide the papers by week. This will be changed later on. A more suitable focus for this survey would be the Energy vs performance sub-domain of runtime and performance analytics. To explore this domain we have summarized some initial papers.

The survey on performance vs energy efficiency focuses on the following research questions:
**RQ1** What is the current state of art?
**RQ2** What is the current state of practice?
**RQ3** What are the challenges of the future work?

To answer these questions three papers are selected to form the basis of this literature survey:

- Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and detecting performance bugs for smartphone applications. In Proceedings of the 36th International Conference on Software Engineering (ICSE 2014). ACM, New York, NY, USA, 1013-1024. DOI: https://doi.org/10.1145/2568225.2568229

- Rui Pereira, Pedro Simão, Jácome Cunha, and João Saraiva. 2018. jStanley: placing a green thumb on Java collections. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE 2018). ACM, New York, NY, USA, 856-859. DOI: https://doi.org/10.1145/3238147.3240473

- Stefanos Georgiou, Maria Kechagia, Panos Louridas, and Diomidis Spinellis. 2018. What are your programming language's energy-delay implications?. In Proceedings of the 15th International Conference on Mining Software Repositories (MSR '18). ACM, New York, NY, USA, 303-313. DOI: https://doi.org/10.1145/3196398.3196414

By looking into the state of programming languages in regards to energy performance, the state of the art will be determined, thus answering RQ1. For the current state of practice (RQ2) literature on the topic of energy efficiency in Android applications will be used. From both topics the challenges and related work will be used for answering RQ3.

**Running list of domain keywords:**
Programming Languages, Energy-Delay-Product, Energy-Efficiency, Empirical study, performance bug, testing, static analysis, Green Software, Energy-aware Software, JCF

###What Are Your Programming Language’s Energy-Delay Implications?
Motivated by the lack of studies that investigate the energy consumption of software applications compared to the number of studies in the energy efficiency of hardware, the researchers set out to investigate the run-time performance of commonly used programming tasks in different languages on different platforms. The paper contributes by giving a customized and extended data set that can be used as a benchmark for similar studies, a set of publicly available tools for measuring the Energy Delay Product (EDP) of various programming tasks implemented in different programming languages, an empirical study on programming language EDP implications, by using different types of programming tasks and software platforms, and a programming language-based ranking catalogue, in the form of heat maps, where developers can find which programming language to pick for particular tasks and platforms; when energy or run-time performance are important. The research questions which are answered are as follows:
Which programming languages are the most EDP efficient and inefficient for particular tasks?
Which types of programming languages are, on average, more EDP efficient and inefficient for each of the selected platforms (i.e. server, laptop and embedded system)?
How much does the EDP of each programming language differ among the selected platforms?
To answer these questions the Rosetta Code Repository, a publicly available repository for programming tasks, is used. It offers 868 tasks, 204 draft tasks and has implementations in 675 programming languages.
The results of the paper are that for most tasks the compiled programming languages outperform the interpreted ones. 

**Keywords:** Programming Languages; Energy-Delay-Product; Energy-Efficiency


###Characterizing and Detecting Performance Bugs for Smartphone Applications
Bugs can cause significant performance degradation, which in turn may lead to losing the competitive edge for the application. The paper is motivated by people having little understanding for performance bugs and the lack of effective techniques to fight these bugs. In the paper the questions are researched what the common types of performance bugs are in Android applications, and what impact they have on the user experience (RQ1), how the performance bugs manifest themselves and if their manifestation needs special input (RQ2), if performance bugs are more difficult to debug and fix compared to non-performance bugs and what information or tools can help with that (RQ3) and if there are common causes of performance bugs, and if patterns can be distilled to facilitate performance analysis and bug detection (RQ4).
Answering these questions leads to the paper making two major contributions:
The first empirical study of real-world performance bugs in smartphone applications. The findings can help understand characteristics of performance bugs in smartphone applications, and provide guidance to related research.
The implementation of a static code analyzer, PerfChecker, which successfully identified performance optimization opportunities in 18 popular Android applications.
The selected Android applications needed to have more than 10.000 downloads and own a public bug tracking system. Furthermore there should be at least hundreds of code revisions. These criteria provide an indicator of the popularity and maturity of the selected applications. At first 29 Android applications were selected, with PerfChecker successfully detecting 126 matching instances of the bug patterns in 18 of these applications. Of these detected 126 matching instances of performance bug patterns, 68 were quickly confirmed by developers as previously unknown issues that affect application performance.

**Keywords**: Empirical study, performance bug, testing, static analysis.

###jStanley: Placing a Green Thumb on Java Collections
In this short paper the tool jStanley is presented. With the help of this tool developers can obtain information and suggestions on the energy efficiency of their Java code. jStanley is available as Eclipse plugin. In a preliminary evaluation jStanley shows energy gains between 2% and 17%, and a reduction in execution time between 2% and 13%.

**Keywords:** Green Software, Energy-aware Software, JCF, Eclipse Plugin

###A Study on the Energy Consumption of Android App Development Approaches
In this study, an analysis is given of the energy consumption of Android app according to which development method was used to create them. They look mainly at the difference between programming languages and their respective frameworks. They measured across multiple devices, which presented little difference between them. They also rewrote some app to use a hybrid framework in the hopes of improving the performance vs Energy consumption balance and they report a non-negligible improvement.

**Keywords:** Android, runtime, performance (search keywords, the paper itself did not contain keywords)


