# Runtime and Performance Analytics

In this chapter, we will discuss the field of performance and runtime analytics. We will not cover the entire field because it is too broad. Using Kitchenham’s method [1], we have narrowed down the scope of this survey. 

For inspiration, we started reading five recent papers on runtime and performance analytics published at top conferences. These five were selected because the papers handle the software side of performance and runtime analytics which is more in line with the other chapters of this book. 
However, focussing on only software, the field is still very broad. Currently, we are leaning towards a focus on performance and runtime analytics literature regarding the Android platform. As we still are at the start of this research, we might deviate from this initial focus.

We have gathered a few other papers (excluding the five initial papers) to find out if this field is suited for this survey. These papers can be found in Figure INSERT FIGURE NUMBER HERE OR SOMETHING.
To get relevant papers, we used the following keywords: Android, performance, runtime, reliability, synchronization, security, monitoring. Furthermore, we only retrieved papers published at top conferences, which we list here:
ACM Transactions on Software Engineering Methodology (TOSEM), 
Empirical Software Engineering (EMSE), 
IEEE Transactions on Software Engineering (TSE), 
Information and Software Technology (IST), 
Journal of Systems and Software (JSS), 
ACM Computing Surveys (CSUR),
Foundations of Software Engineering (SIGSOFT FSE),
International Conference on Automated Software Engineering (ASE),
Working Conference on Mining Software Repositories (MSR) 
Symposium on Operating Systems Design and Implementation (OSDI)

Because we consider the five starting papers to be our inspiration, we have chosen to briefly describe these papers by giving some basic metrics about them (citations), summarizing them and by adding a few notes about them. This is our initial work that we would like to expand on in the coming weeks.


## Charting the API minefield using software telemetry data

Year: 2015, published in top journal. 
Citations: 
scholar: 9
scopus: 4

In this paper, researchers used software telemetry data from mobile application crashes. With heuristics, they separated the API calls from application calls so they can analyze what the most common causes for crashes are. Top crash causes are: memory exhaustion, race conditions or deadlocks, and missing resources. A significant percentage was not suitable for analysis as these crashes were associated with generic exceptions (10%). They performed a literature search to find solutions to the problems that cause the crashes. For each crash cause category, an implementation recommendation is made. More specific exceptions, non-blocking algorithms, and default resources can eliminate the most frequent crashes. They also suggest that development tools like memory analyzers, thread debuggers, and static analyzers can prevent many application failures. They also propose features of execution platforms and frameworks related to process and memory management that could reduce application crashes.

Remarks

* Among the papers that refer to this paper or are referenced by this paper there are four papers that share the topic of crash data on mobile platforms that have been publiced to top software engineering venues. 
* The paper seems to be quite discerning as they evaluate their methods and reason about the threats to validity. 
  

## Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring

Year: 2016, published in top conference. 
Citations: 
scholar: 18
scopus: 10

The mobile applications market continues to grow and many applications are available. It is important for developers that their application keeps working and that crashes are fixed as fast as possible to keep up with competitors. However, the mobile market is tough: there are many different versions/hardware/steps end users take and it is hard to reproduce the environment and steps the end users take before running into a crash. This is why the researchers developed MoTiF which uses machine learning to reproduce the steps the end users take before the app crashes on the end user’s phone and generates a test suite. MoTiF also uses the crowd to validate whether the generated test suite truly reproduces the observed crash.

Remarks

* The datasets used for the research are a bit questionable. One is based on simply performing a large amount of random event on the app, the other dataset is created by letting a group of 10 student try to crash the app in one hour. 
* Only 5 different apps have been tested. 
* Contains reference to "Charting the API minefield using software telemetry data". 

## An exploratory study on faults in web api integration in a large-scale payment company

Year: 2018, published in top conference. 
Citations: 
scholar: 2
scopus: 0

This research explores what the implications of web API faults are, what the most common web API faults are and best practices for API design. The faults in API integration can be grouped in 11 causes: invalid user input, missing user input, expired request data, invalid request data, missing request data, insufficient permissions, double processing, configuration, missing server data, internal and third party. Most faults can be attributed to the invalid or missing request data, and most API consumers seem to be impacted by faults caused by invalid request data and third party integration. Furthermore, API consumers most often use official API documentation to implement an API correctly, followed by code examples. The challenges of preventing problems from occurring are the lack of implementation details, insufficient guidance on certain aspects of the integration, insufficient understanding of the impact of problems, and missing guidance on how to recover from errors.

Remarks

* Easy to read
* Paper only considers a single API
* Survey only has 40 responses

## Search-based test data generation for SQL queries

Year: 2018, published in top conference. 
Citations: 
scholar: 0
scopus: 0

SQL queries should be tested as thoroughly as program code. However, it is hard to generate test data for testing. Other researchers proposed viewing this problem as a constraint solving problem, so test data could be generated with a SAT-solver. However, strings are not supported by current SAT-solver tools and it is a complex task to translate a query to a satisfiability problem. In this research, they view the test generation problem as a search-based problem. They use random search, biased random search and genetic algorithms (GA) to generate the data. The methods are combined in a tool called EvSQL and the tool is tested on more than 2000 queries. The GA method is the best and is able to cover a little over 98% of the queries. 

Remarks

* Easy to read
* Utilizes queries of 4 different systems
* Generation of test data for SQL questies implies easier generation of unit- regression- and integration tests for SQL queries. 

## Anomaly detection using program control flow graph mining from execution logs

Year: 2016, published in top conference. 
Citations: 
scholar: 12
scopus: 8

The paper attempts to diagnose distributed applications. For this purpose they mine templates and their sequences from exedcution logs, from this information they create a control flow graph. The main cause of failures identified: making an API request to another application. This results in many new calls to other services or even other applications. This flow gets interrupted at some point. So when the top level API is not working, they want to show where it goes wrong. In earlier work, primarily metrics and logs were used to find the cause. Drawbacks of this: many benign warnings or errors in healthy state or faults do not manifest as errors. Manually checking a transaction flow is also very hard. They use templates: print statements from the source code. These are the nodes, the edges are the flows. Two challenges: mining print statements is hard because parameters are different in every log and flows can happen at the same time. Their method: they do a join on two print statements if the statements are preceded and followed by approximately the same steps. 

Remarks 

* Has a presentation on YouTube
* Difficult to read
