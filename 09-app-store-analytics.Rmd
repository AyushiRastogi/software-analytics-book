# App Store Analytics

## Motivation

In the year 2008, the first app stores became available. These marketplaces have grown rapidly in
size since then with over 3.8 million apps in the Google Play store alone as of first quarter of
2018 [@appNumber]. These app stores together with the large user base associated with them provide
software developers and researchers with valuable data. The process of using the data from the app
stores to gain valuable insights is what we would call “App Store Analytics”. Because these app
marketplaces are relatively new, the research field of App Store Analytics is still not mature.
However, because apps are used so much nowadays, it plays an important role in the field of
Software Engineering.
 
In 2015, Martin, William, et al., published "A survey of app store analysis for software
engineering." In the paper, 7 main categories are proposed of the field: API Analysis, Review
Analytics, Security, Store Ecosystem, Size and Effort Prediction, Release Engineering and Feature
Analysis @martin2015survey. Given that relevant literature on the field of App Store Analytics has
been already gathered and analyzed, it makes sense to use this chapter to go a step further.  In
this order of ideas, we will delve deeper into the subfield of Review Analysis. We check the
literature proposed by @martin2015survey and update it with the relevant articles (as defined by
the inclusion criteria presented below) that were published after 2015.  We then use the data to
answer the following research questions regarding the chosen subcategory.

* **RQ1** What is the state of the art in Review Analysis? Specifically:
    - Which topics that are being explored?
    - Which research methods, tools, and datasets being used?
    - Which are the main research findings (aggregated)?

* **RQ2** What is the state of practice in Review Analysis? Specifically:
    - Which are the tools and the companies that create/employ them?
    - Case studies and their findings.

* **RQ3** Which are the challenges and future research that the field is facing or will face?

## Research Protocol

In this section we present the process that we followed to systematically extract the articles for
the literature survey. First, the search strategy includes the queries that were used, as well as
the initial criteria that was taken into account for the first filtering of the articles. Then, the
way how the relevance of the remaining papers was assessed and the second filtering are explained
in the article selection section. In the last two parts, we describe the data that was extracted
from each study and the initial grouping that was constructed based on this.

### Search Strategy

As stated in the motivation, the survey by Martin et al @martin2015survey gave us a starting point.
Instead of gathering everything related to App Store Analytics we decided to focus on expanding the
work done by the authors and to answer the research questions specifically for the
subcategory of Review Analytics. All the papers that @martin2015survey
found related Review Analytics were retrieved and inspected to get a sense of their specific
keywords and content. 

After doing this, the following search queries were generated:
```
“app store analytics” 
“app store analytics” AND mining 
“app store analytics” AND “user reviews” 
“app store analytics” AND “reviews”, “app reviews”
```

Google Scholar, ACM Digital Library, and IEEE Xplore were used for the searching process with the
aforementioned queries. In order to keep a database with only the relevant articles, the following
inclusion criteria were applied to all the hits. It is important to notice that in addition to the
results obtained by searching with the specific query, the first page of the “related articles”
link for the top cited articles was also inspected. 

| Criteria | Value        | 
|----------------|-------------|
| time frame    | 2015-present |
| journals and conferences | TSE, EMSE, JSS, IST, ICSE, FSE, MSR, OSDI, MobileSoft |
| keywords in title | user reviews OR app store reviews |
| keywords in abstract | user reviews OR app store reviews |

After the relevant articles were selected, their metadata was gathered, and they were included in a
database. This was the input for the next step of the survey, article selection.

### Article selection
Taking into account that the papers considered in this survey were published from 2015, it is no
surprise that most of them are not highly cited. As a consequence, the selection of the filtered
articles does not take this into consideration. In contrast, each member of the group was in charge
of delving into a third of the database and finding, for each study, the *relevance* with respect
to Review Analytics and the proposed research questions. Then, this score was peer-reviewed by
another team member to achieve a consensus. 

For the *relevance* score, we considered how much the paper actually involved the analysis
of user reviews. Next, we present three examples: 1) a highly relevant paper (score=10), 2) a
somewhat relevant paper (score=5) and 3) a non-relevant paper (score=0). 

**What would users change in my app? summarizing app reviews for recommending software changes
[@di2016would]** 

 - Relevance score: 10
 - Remarks: the authors applied classification and summarization techniques on app reviews to
   reduce the effort required to analyze feedback from users. As can be seen, the paper was focused
   on using the reviews to improve the development process.

**Fault in your stars: an analysis of Android app reviews [@aralikatte2018fault]** 

- Relevance score: 5
- Remarks: the authors analyzed the problem of the potential mismatch between the app reviews and
  the star ratings that the app receives. Although it is related to reviews it is not its main
  focus. It also does not push the state of the art in terms of the datasets that they used. 

**Why are Android apps removed from Google Play? A large-scale empirical study
[@wang2018android]** 

- Relevance score: 0
- Remarks: in this case, the paper did not have to do with app reviews even though the title
  suggested otherwise.

In the end, only the articles that had a score of 5 or more were used for the fact extraction and
the subsequent answering of the research questions.

### Fact extraction

As was mentioned before, the articles were indexed in a database in a structured fashion. The data
that was extracted has the following fields:

```
id for indexing
title
year 
relevance score 
relevance description 
source 
category
authors Information
source (journal or conference)
complete reference
```
Additionally, for each one of the articles, a systematic reading was carried in which bullet points
that answered the following questions were generated:

```
Paper type
Research question of the paper
Contributions
Datasets: size and sampling methodologies
Techniques used for doing the analysis 
```

### Initial grouping of papers

TODO

## Answers 

* **RQ1** What is the state of the art in Review Analysis? Specifically:
    - Which topics that are being explored?
    - Which research methods, tools, and datasets being used?
    - Which are the main research findings (aggregated)?

To answer thequestion at hand we looked at the novel ideas and the research that has been done in
this field since that time. In their survey, @martin2015survey proposed “Classification”,
“Content”, “Requirements Engineering”, “Sentiment”, “Summarization” and “Surveys and Methodological
Aspects of App Store Analysis” to categorize the existing literature. After analyzing posterior
work, we propose new categories that reflect better the state of the art in this field. These are:
“Review Manipulation”, “Mapping user reviews to the source code”, “Privacy / App Permissions”,
“Responding to reviews”, “Comparing Apps and App stores and Wearables”. In the following sections,
we expand on each one of these with the revised literature.

**Review Manipulation**

Recently, significant attention has been paid to how the reviews and ratings can be used to
influence the number of downloads of a particular app in the App Store. In their paper,
@i2017crowdsourced analyzed the use of crowdsourcing platforms such as Microworkers to manipulate
the ratings. The authors merged data from two different sources, App Store and crowdsourcing site,
to identify manipulated reviews.

In @chen2017toward the authors propose an approach to identify attackers of collusive promotion
groups in the app store. They use ranking changes and measurements of pairwise similarity to form
targeted app clusters (TAC) that they later use to pinpoint attackers. A different approach to the
same problem was proposed by @xie2016you. In the paper, the authors identify manipulated app
ratings based on the so-called attack signatures. They present a graph-based algorithm for
achieving this purpose in linear time. 

These papers also show that the percentage of manipulated apps in the app stores is not the
majority of the used samples (@xie2016you found that less than 1% of the apps were suspicious).
Regarding the datasets, the work by @i2017crowdsourced used the smaller amount of app store data.
Considering that they manage to merge it from data from an external crowdsourcing site. In the case
of the other two papers, they can be characterized by the small number of considered apps (compared
to the total number of apps in the main marketplaces), but the significant number of reviews that
were analyzed. This makes sense, considering that the main purpose is to investigate the later. 

Challenges & Future Research:
* It is important to mix multiple sources of data in order to better identify suspicious
  individuals. These include not only app stores, but also crowdsourcing sites and even social
  networks. 
* As the number of suspicious apps is not large, taking into account the size of the marketplace,
  is important to correctly select the sample so there is enough information for the algorithms to
  “learn”. 

**Requirements Engineering**
TODO

**Mapping user reviews to source code**
TODO

**Privacy / app permissions**
TODO

Since Android Marshmallow the Android operating system uses a run-time permission-based security
system. Apple's iOS also uses run-time permissions on top of a set of permissions enabled by
default. Scoccia et al. did a large-scale empirical study on this new system by inspecting 4.3
million user reviews from 5572 Google Play store apps [@scoccia2018investigation]. Using different
techniques they extracted 3574 user reviews that relate to permissions. They found that users like
the minimal permissions as most apps only ask for permissions they strictly need. Some of the
negative user concerns were apps asking for too many permissions or a bad timing asking for
permissions.

Challenges & Future Research:

* A

**Responding to reviews**

Since 2013 developers can respond to reviews in the Google Play store and Apple introduced the same
feature in 2017. In previous work, McIlroy [@mcilroy2014empirical] studied whether responding to
user reviews has a positive effect on the rating users give. Building on previous work McIlroy et
al. studied how likely it is for users to change their rating for an app when a developer responds
to their review [@mcilroy2017worth]. They found that users change
their rating 38.7 percent of the time when a developer responds to their review, with a median
increase of 20 percent in rating for the app.

Hassan et al. [@hassan2018studying] used 2328 top free apps from the Google Play store to study
whether users are more likely to update their review if a developer responds to their review. They
extracted 126686 dialogues between developers and users and concluded that responding to a review
increases the chances of users updating their given rating for an app can increase by up to six
times when compared to not responding. Next, to this they also studied the characteristics,
likelihood, and the incentives of user-developer dialogues in app stores.

Challenges & Future Research:

* in-depth study of how developers and users are using the review mechanism to find out how the
  mechanism can be improved.
* Investigate whether a limit of 500 responses is sufficient to ban useless responses from the
  store such as thanking every user.
* Clustering reviews

**Comparing Apps and App stores**

Papers that compare apps or app stores are discussed in this section.  Li et  al. mined user
reviews from Google Play to find comparisons between apps [@li2017mining]. They set out to identify
comparative reviews to extract differences between apps on different topics. For example, a user
says in a review that this app is not as good regarding power consumption than another app. Li et
al. created a method that with sufficient accuracy extracts these opinions and provides comparisons
between apps. 

Ali et al. did a comparative study on cross-platform apps. They took a sample of 80.000 app-pairs
to quantitatively compare the same apps across different platforms and identify the differences
between the platforms. [@ali2017same]  
In a related study Hu et al. compared app-pairs that are created using hybrid development tools
such as PhoneGap. [@hu2018studying] With this approach they found that in 33 of the 68 app-pairs
the star rating was not consistent across platforms. 

Challenges & Future Research:

* One of the open challenges is including indirect relationships in the comparisons as only direct
  relationships were used in work by Li et al.
* Hu et al. noticed that apps that have been developed using hybrid development tools had
  relatively low ratings. Future studies should be done to investigate the quality of hybrid apps
  and compare them with the quality of native apps.
* A detailed approach grouping topics with more granularity.
* Market-scale analysis using a large number of apps and reviews.  
* Testing and analysis of apps across multiple platforms and hardware.

**Wearables**

* **RQ2** What is the state of practice in Review Analysis? Specifically:
    - Which are the tools and the companies that create/employ them?
    - Case studies and their findings.

* Best practices for responding to reviews based on millions of reviews:
  https://support.appbot.co/help-docs/best-practices-replying-to-ios-and-google-play-reviews/.
  There are also tools such as AppSee which contain guides for responding to reviews and perform
  sentiment analysis of the user reviews. 
* To this day there are, to the best of our knowledge, no tools that compare apps and app stores
  using user reviews. There are tools that analyze apps and app performance, but no tools for
  comparing.

TODO

* **RQ3** Which are the challenges and future research that the field is facing or will face?

In this section, we present challenges and future trends for the field of Review Analytics and the
subcategories that were identified in previous sections. 

An important component of the research process is the validation of the proposed tools and
frameworks and the assessment of how generalizable they are. For this, it is necessary to have
bigger reviews datasets and more representative and robust samples. In terms of the techniques
being used, machine learning is a being trend and in the future approaches that are better tailored
to the problem at hand must be devised. Also, most of the studies rely on correlation relationships
to validate the effectiveness of their approaches. There is a need to apply causation techniques so
confounding factors can be ruled out.

Taking into account the practical aspect of reviews analytics, there is a difficulty to translate
the research into actual tools that are used in real settings. Of all the works that were
considered, only @gao2018online by Gao et al. presented a case study and most of the tools have
either disappeared or are inactive.

*Review Manipulation:*
It is important to combine multiple sources of data to better identify suspicious individuals.
These include not only app stores, but also crowdsourcing sites and even social networks. Also, the
sample should be carefully selected, given that the number of suspicious apps is not large (around
1% of all apps), taking into account the size of the app stores.

*Requirements Engineering:* 

*Mapping user reviews to source code:*
There is a need to better understand the “language of source code” as this will be important to
join the reviews and source code datasets. A likely future trend is the analysis of update-level
changes. Regarding this, there is still a need to obtain update-categorized reviews as this is
still a challenge with the current review-retrieving approaches.

*Privacy / app permissions:*
*Responding to reviews:*
*Comparing Apps and App stores:*

TODO

## Appendix

## Paper’s Extracted Data

### Li, Shanshan, et al. "Crowdsourced App Review Manipulation." Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2017.

**Complete reference:** @i2017crowdsourced

- Analysis of apps that have manipulated by reviews. Specifically the use of crowdsourcing
  platforms to achieve this goal. 
- Took a sample of 114 targeted apps from a crowdsourcing platform to run their analysis. And also
  a randomly selected control group from the App Store. 
- Used data from another site (Microworkers) and matched with apps data.
- The goal of manipulating ratings is to promote one’s own app or to demote competitors? The
  analysis shows the former is true. 
- Analyzed time distribution of crowdsourced rating. They exhibited a burstiness after the
  crowdsourcing task was posted.
- They used a similarity measure between reviews (Jacquard similarity) to find out how similar
  suspicious reviewers were to each other. 
- Used divergence measure (Kullback-Leibler) to consider reviews between related apps and apps for
  the same developer.
- App sample needs to be enlarged and made more representative in order to draw definitive
  conclusions. 
- There is an interest in incorporating machine learning into the analysis.

### Chen, Hao, et al. "Toward detecting collusive ranking manipulation attackers in mobile app markets." Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017.

**Complete reference:** @chen2017toward

- Approach to identify attackers of collusive promotion groups in an app store. 
- Use app ranking changes, measure pairwise similarity, forms targeted app clusters (TACs), and
  identifies the collusive group members. 
- Used a sample of Apple’s China App Store to test the approach. 37200 records with totally 6651
  applications. After filtering the apps with low values of drastic ranking change frequency (DRCF)
  the size of the sample was 461 apps. 

### Xie, Zhen, et al. "You can promote, but you can't hide: large-scale abused app detection in mobile app stores." Proceedings of the 32nd Annual Conference on Computer Security Applications. ACM, 2016.

**Complete reference:** @xie2016you

- Identification of manipulated app ratings by analyzing attack signatures. 
- Development of an algorithm (graph-based) to identify these attackers in linear time.
- In the initial validation, they used 2188 apps with 4841720 reviews and 1622552 reviewers. They
  ran the tool for 33 hours and 31 minutes. 
- Analyzed app stores from different markets: China, United Kingdom, and United States of America.
- Suspected apps are around 1% of the total number of apps.
- Possible deployment of the algorithm by app store vendors or a third party.

### Di Sorbo, Andrea, et al. "What would users change in my app? summarizing app reviews for recommending software changes." Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 2016.

**Complete reference:** @di2016would

- Automated approaches with the aim of reducing the effort required for analyzing feedback
  contained in user reviews via automatic classification/prioritization according to specific
  topics @di2016would.
- Introduced SURF (Summarizer of User Reviews Feedback). An approach to condense a large number of
  user reviews. Main features are summarization and classification.
- Extract useful information for developers doing maintenance and evolution tasks. 
- To validate the output of SURF, the authors realized 2 experiments using 17 apps and 23
  developers and researchers. 
- There is a need to use a larger and representative sample of apps to test the validity of the
  model.

### Nayebi, Maleknaz, Henry Cho, and Guenther Ruhe. "App store mining is not enough for app improvement." Empirical Software Engineering (2018): 1-31.

**Complete reference:** @nayebi2018app

- Complements app reviews with other sources of information such as Twitter. In this sense, authors
  use more than one feedback mechanism for the app analysis. 
- Is there a relationship between app store ratings and Twitter engagement with respect to a
  certain app? Which are the differences between tweets and app store reviews’ provided
  information? In general how tweets compare to reviews
- App Store information is not enough, it can be complemented with other sources.
- The used sample contained 30793 Google Play apps that vary in price, category, popularity,
  rating, and number of reviewers. 
- Authors describe the methodology of extracting app related content from Twitter. 
- Used statistical tests, NLP techniques such as topic modeling and machine learning.

### Simmons, Andrew, and Leonard Hoon. "Agree to disagree: on labelling helpful app reviews." Proceedings of the 28th Australian Conference on Computer-Human Interaction. ACM, 2016.

**Complete reference:** @simmons2016agree

- Explored the “helpfulness” of app reviews. 
- Used a pre-made dataset of app reviews (Socrates App Review Dataset). This contains App Store
  reviews from the top 400 free and top 400 paid apps of each category. Then from this, selected a
  random sample of 167 reviews from the Health & Fitness category. 
- Surveyed users about the helpfulness of 167 reviews. It amounts to 2558 helpfulness ratings. Used
  Likert scale for surveys.
- The authors only found slight agreement with respect to helpfulness among participants.
  Disagreement among users limits the potential of mobile app review recommender systems. 
- Sample selection is a topic that still needs study as its usually based on authors preconceptions
  and intuitions rather than in a sound method. 

### Aralikatte, Rahul, et al. "Fault in your stars: an analysis of Android app reviews." Proceedings of the ACM India Joint International Conference on Data Science and Management of Data. ACM, 2018.

**Complete reference:** @aralikatte2018fault

- Study of the potential mismatch between an app review and its associated rating.
- Manually examined 8600 reviews from 10 popular android apps. For apps with fewer reviews these
  mismatches can strongly alter their ratings.
- Use of machine learning and deep learning models to identify the mismatch. 
- They categorize things that are subjective. For instance, when a review should be accompanied by
  a certain number of stars. 
- Assembled large training sets for the classification algorithms by hand.
- Use of external evaluators to corroborate the results of their automatic approach.

### Gao, Cuiyun, et al. "Online app review analysis for identifying emerging issues." 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018.

**Complete reference:** @gao2018online

- Propose a new framework (IDEA) for identifying emerging app issues effectively based on an online
  review analysis. 
- Propose a method called AOLDA for online review analysis, combining the topics from previous
  versions. 
- They include a preprocessing step, topic detection, topic interpretation and visualization of the
  results.
- A sample of 6 popular apps from (4) Google Play and (2) App Store. Employed app’s changelogs as
  the ground truth.
- The presented framework is being used in several Tencent products.
- Conducted an actual survey in Tencent to corroborate the validity of their findings. 

### Panichella, Sebastiano, et al. "Ardoc: App reviews development oriented classifier." Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 2016.

**Complete reference:** @panichella2016ardoc

- Present a tool for automatically classifying useful feedback contained in app reviews in the
  context of software maintenance and evolution. 
- Used NLP, text analysis and sentiment analysis. 
- Used 3 applications for testing the tool. Compared human labels with the ones obtained in the
  tool. 

### Anchiêta, Rafael T., and Raimundo S. Moura. "Exploring Unsupervised Learning Towards Extractive Summarization of User Reviews." Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web. ACM, 2017.

**Complete reference:** @anchieta2017exploring

- An unsupervised approach to categorize the reviews aiming to generate a summary of the main bugs
  and new features pointed by users. 
- Clustered the reviews and used an algorithm for topic modeling. 
- They evaluated their approach using 11 apps from the communication category on Google Play Store.
  The fact that the dataset is small may pose challenges to the generalization of the results. 
- App stores reviews are noise, especially with bigger apps such as WhatsApp. Therefore
  preprocessing of the data for obtaining the most relevant ones is important to ensure a
  good performance of the algorithms. 

### What would users change in my app? summarizing app reviews for recommending software changes

**Complete reference:** @di2016would

**Source:** Proceeding FSE 2016 Proceedings of the 2016 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering

**Main topic area:** using user feedback/reviews

**Authors information (full names, institution, and country):**
- Andrea Di Sorbo - University of Sannio, Italy
- Sebastiano Panichella - University of Zurich, Switzerland
- Carol V. Alexandru - University of Zurich, Switzerland
- Junji Shimagaki - Sony Mobile Communications, Japan
- Corrado A. Visaggio - University of Sannio, Italy
- Gerardo Canfora - University of Sannio, Italy
- Harald C. Gall - University of Zurich, Switzerland

**Summary (research questions and answers):** The paper proposes a new approach to analyzing App
Store user reviews, deriving insights from them. The presented solution has two components. First,
the User Reviews Model (URM) that enable the classification of users intentions (e.g., UI
improvements, bug fixes, etc.). Second, the Summarizer of User Review Feedback (SURF). A tool that,
by leveraging the URM, is capable of generating summaries of users feedback.  After evaluating the
proposed approach, TODO

**Research question/issue:** there is no approach that is able to do, at the same time, the
following: (i) determine for a large number of reviews the specific topic discussed in the review
(e.g., UI improvements, security/licensing issues, etc.), (ii) identify the maintenance task to
perform for addressing the request stated in the review (e.g., bug fixing, feature enhancement,
etc.), and (iii) present such information in the form of a condensed, interactive and structured
agenda of recommended software changes, which is actionable for developers. [@di2016would]


### API change and fault proneness: A threat to the success of Android apps

**Source:** Conference ESEC/FSE'17 Joint Meeting of the European Software Engineering Conference
and the ACM SIGSOFT Symposium on the Foundations of Software Engineering

**Main topic area:** using user feedback/reviews, API changes

**Authors information (full names, institution, and country):** - Mario Linares-Vásquez -
Universidad de los Andes, Colombia - Gabriele Bavota - University of Sannio, Italy - Carlos
Bernal-Cárdenas - Universidad Nacional de Colombia, Colombia - Massimiliano Di Penta - University
of Sannio, Italy - Rocco Oliveto - University of Molise, Italy - Denys Poshyvanyk - College of
William and Mary, USA

The paper presents an empirical study that aims to corroborate the relationship between the fault
and change-proneness of APIs and the degree of success of Android apps measured by their user
ratings. For this, the authors selected a sample of 7,097 free Android apps from the Google Play
Market and gathered information on the changes and faults that the APIs used by them presented.
Using this data and statistical tools such as box-plots and the Mann-Whitney test, two main
hypotheses were analyzed. The first hypothesis tested the relationship between fault-proneness
(number of bugs fixed in the API) and the success of an app. The second tested the relationship
between change-proneness (overall method changes, changes in method signatures and changes to the
set of exceptions thrown by methods) and the success of an app. Finally, although no causal
relationships between the variables can be assumed, the paper found significant differences in the
level of success of the apps taking into consideration the change and fault-proneness of the APIs
they use. 


**Research question/issue:** a relationship between fault- and change-proneness of APIs and the
degree of success in Android apps.

### App Store, Marketplace, Play! An Analysis of Multi-Homing in Mobile Software Ecosystems
 
**Source:** Proceedings of the Fourth International Workshops on Software Ecosystems **Main topic
area:** App store ecosystem

**Authors information (full names, institution, and country):** Sami Hyrynsalmi, University of
Turku, Finland
 
Tuomas Mäkilä, University of Turku, Finland
 
Antero Järvi, University of Turku, Finland
 
Arho Suominen, VTT Technical Research Centre of Finland, Finland
 
Marko Seppänen, Tampere University of Technology, Finland
 
Timo Knuutila, University of Turku, Finland

**Summary (research questions and answers):** Multi-homing is not used by many developers, where
multi-homing is the strategy of releasing your application to multiple platforms. An analysis of
Google Play, App Store and Windows Phone Store shows that not many developers use this strategy.
Next to this, the paper found that the type and popularity of apps does not differ from those that
use a single-homing strategy.
 
**Research question/issue:** Analysis of multi-homing in different app stores. How much is it used
by developers and is there a difference in popularity?

### A systematic literature review: Opinion mining studies from mobile app store user reviews

**Source:** Journal of Systems and Software
 
**Main topic area:** Opinion Mining and Requirement Engineering
 
**Authors information (full names, institution, and country):** Necmiye Genc-Nayebi, École de
Technologie Supérieure (ETS) - Université du Québec, Canada Dr. Alain Abran,  École de Technologie
Supérieure (ETS) - Université du Québec, Canada
 
**Summary (research questions and answers):** TODO: summary
 
**Research question/issue:** What are the proposed solutions for mining online opinions in app
store user reviews, challenges and unsolved problems in the domain, new contributions to software
requirements evolution and future research direction.
